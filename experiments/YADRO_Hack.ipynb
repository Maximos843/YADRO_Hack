{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "reXSLPwrWWCn",
        "3-ftuW4H4iAY",
        "_Qk6EHkL8yGa",
        "_nduHgY4FtpW",
        "o0EKKOBDR6_c",
        "JdHrtb61uR9S"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVkzveMRUpkd"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create df"
      ],
      "metadata": {
        "id": "reXSLPwrWWCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "i8r1EzTzVahe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temps = np.load('/content/drive/MyDrive/YADRO_Hack/temperature.npy')\n",
        "clouds = np.load('/content/drive/MyDrive/YADRO_Hack/cloud_cover.npy')\n",
        "humidity = np.load('/content/drive/MyDrive/YADRO_Hack/humidity_forecast5_2.npy')\n",
        "elevation = np.load('/content/drive/MyDrive/YADRO_Hack/elevation.npy')\n",
        "pressure = np.load('/content/drive/MyDrive/YADRO_Hack/pressure.npy')\n",
        "wind_dir = np.load('/content/drive/MyDrive/YADRO_Hack/wind_dir.npy')\n",
        "wind_speed = np.load('/content/drive/MyDrive/YADRO_Hack/wind_speed.npy')"
      ],
      "metadata": {
        "id": "o8qgBdhdVajz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#датасет разбит на диапазоны по часу измерения, каждый час имеет 900 строк для каждой из координат с соттветсвующими признаками\n",
        "df = pd.DataFrame(data=[[hour, row, col, elevation[row][col], clouds[hour][row][col], humidity[hour][row][col],\n",
        "                         pressure[hour][row][col], wind_dir[hour][row][col],\n",
        "                         wind_speed[hour][row][col], temps[hour][row][col]]\n",
        "                        for hour in range(43) for row in range(30) for col in range(30)],\n",
        "                  columns=['hour', 'row', 'col', 'elevation', 'cloud', 'humidity', 'pressure', 'wind_dir', 'wind_speed', 'tempreture'])\n",
        "df"
      ],
      "metadata": {
        "id": "YVvDaD5zVaov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.elevation.max(), df.elevation.min()"
      ],
      "metadata": {
        "id": "wob1-e_pj8Wv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check temps series for one value"
      ],
      "metadata": {
        "id": "3-ftuW4H4iAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_1_1 = pd.DataFrame(data=[[hour, 0, 0, elevation[0][0], clouds[hour][0][0], humidity[hour][0][0], pressure[hour][0][0],\n",
        "                             wind_dir[hour][0][0], wind_speed[hour][0][0], temps[hour][0][0]] for hour in range(43)],\n",
        "                      columns=['hour', 'row', 'col', 'elevation', 'cloud', 'humidity', 'pressure', 'wind_dir', 'wind_speed', 'tempreture'])\n",
        "df_1_1.head()"
      ],
      "metadata": {
        "id": "mzLN472Q4bQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "series = df_1_1['tempreture'].copy()\n",
        "series.plot()"
      ],
      "metadata": {
        "id": "ssMa7qQEfAfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datetime import datetime, timedelta\n",
        "\n",
        "\n",
        "now = datetime(2024, 4, 24)\n",
        "series.index = [pd.to_datetime(now + timedelta(hours=i)) for i in range(43)]\n",
        "series.head()"
      ],
      "metadata": {
        "id": "NOUuUYsTfeH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#по идее ряд стационарный, тк p-value > 0.05 => не отвергаем гипотезу, что ряд стационарный\n",
        "import statsmodels.api as sm\n",
        "\n",
        "\n",
        "sm.tsa.stattools.kpss(series, regression='ct')"
      ],
      "metadata": {
        "id": "r64iASZIkaDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf\n",
        "from statsmodels.graphics.tsaplots import plot_pacf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "f, ax = plt.subplots(nrows=2, ncols=1, figsize=(5, 6))\n",
        "plot_acf(series.values, lags=20, ax=ax[0])\n",
        "plot_pacf(series.values, lags=20, ax=ax[1], method='ols')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r1calkHXljuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "res = sm.tsa.seasonal_decompose(series, model='additive')\n",
        "plt.rc(\"figure\", figsize=(25,8))\n",
        "resplot = res.plot()"
      ],
      "metadata": {
        "id": "_hBDo7YcYJAI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline backtesting_forecaster for independent region"
      ],
      "metadata": {
        "id": "_Qk6EHkL8yGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install skforecast"
      ],
      "metadata": {
        "id": "PZcnJGNA94V8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from skforecast.ForecasterBaseline import ForecasterEquivalentDate\n",
        "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
        "from skforecast.model_selection import bayesian_search_forecaster\n",
        "from skforecast.model_selection import backtesting_forecaster"
      ],
      "metadata": {
        "id": "PvEoQe689szr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "now = datetime(2024, 4, 25)\n",
        "df_1_1.index = [pd.to_datetime(now + timedelta(hours=i)) for i in range(43)]\n",
        "df_1_1 = df_1_1.asfreq('H')\n",
        "df_1_1.head()"
      ],
      "metadata": {
        "id": "h2AHj2MqAj-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#len_val = 5\n",
        "n = 38\n",
        "data_train = df_1_1.iloc[:n, :]\n",
        "data_val = df_1_1.iloc[n:, :]"
      ],
      "metadata": {
        "id": "CgRJM2G58lBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecaster = ForecasterEquivalentDate(\n",
        "                 offset = pd.DateOffset(days=1),\n",
        "                 n_offsets = 1,\n",
        "             )\n",
        "\n",
        "forecaster.fit(y=df_1_1.iloc[:n, -1])\n",
        "forecaster"
      ],
      "metadata": {
        "id": "rkDFnESh9fYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metric_baseline, predictions = backtesting_forecaster(\n",
        "                                   forecaster         = forecaster,\n",
        "                                   y                  = df_1_1['tempreture'],\n",
        "                                   steps              = 5,\n",
        "                                   metric             = 'mean_absolute_percentage_error',\n",
        "                                   initial_train_size = len(df_1_1.values[:n]),\n",
        "                                   refit              = False,\n",
        "                                   n_jobs             = 'auto',\n",
        "                                   verbose            = True,\n",
        "                                   show_progress      = True\n",
        "                               )\n",
        "\n",
        "print(f\"Backtest error (MAE): {metric_baseline}\")"
      ],
      "metadata": {
        "id": "4Y935D8E9fan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "forecaster.predict(steps=5)"
      ],
      "metadata": {
        "id": "a6nlxP4C1apq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLForecast"
      ],
      "metadata": {
        "id": "VS4ODrVMCgRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install mlforecast\n",
        "!pip install catboost\n",
        "#!pip install numba"
      ],
      "metadata": {
        "id": "C4JetbtM9NAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from mlforecast import MLForecast\n",
        "from mlforecast.target_transforms import Differences\n",
        "from mlforecast.utils import PredictionIntervals\n",
        "from window_ops.expanding import expanding_mean\n",
        "from mlforecast.lag_transforms import ExpandingMean, RollingMean, ExponentiallyWeightedMean\n",
        "from mlforecast.target_transforms import LocalStandardScaler\n",
        "\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta\n",
        "from sklearn.metrics import mean_absolute_percentage_error"
      ],
      "metadata": {
        "id": "GCfjBsNO4INl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_df_wind_speed(n: list[tuple]) -> pd.DataFrame:\n",
        "    df = pd.DataFrame(data=[['0/0', hour + 1, wind_speed[hour][0][0], wind_speed[hour][n[0][0]][n[0][1]], wind_speed[hour][n[1][0]][n[1][1]], wind_speed[hour][n[2][0]][n[2][1]],\n",
        "                            wind_speed[hour][n[3][0]][n[3][1]], wind_speed[hour][n[4][0]][n[4][1]], wind_speed[hour][n[5][0]][n[5][1]], wind_speed[hour][n[6][0]][n[6][1]],\n",
        "                            wind_speed[hour][n[7][0]][n[7][1]]]  for hour in range(43)],\n",
        "            columns=['unique_id', 'ds', 'y', 'neibr1', 'neibr2', 'neibr3', 'neibr4', 'neibr5', 'neibr6', 'neibr7', 'neibr8'])\n",
        "    for col in df.columns[3:]:\n",
        "        df[col] = df[col].shift(1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_df_wind_dir(n: list[tuple]) -> pd.DataFrame:\n",
        "    df = pd.DataFrame(data=[['0/0', hour + 1, wind_dir[hour][0][0], wind_dir[hour][n[0][0]][n[0][1]], wind_dir[hour][n[1][0]][n[1][1]], wind_dir[hour][n[2][0]][n[2][1]],\n",
        "                            wind_dir[hour][n[3][0]][n[3][1]], wind_dir[hour][n[4][0]][n[4][1]], wind_dir[hour][n[5][0]][n[5][1]], wind_dir[hour][n[6][0]][n[6][1]],\n",
        "                            wind_dir[hour][n[7][0]][n[7][1]]]  for hour in range(43)],\n",
        "            columns=['unique_id', 'ds', 'y', 'neibr1', 'neibr2', 'neibr3', 'neibr4', 'neibr5', 'neibr6', 'neibr7', 'neibr8'])\n",
        "    for col in df.columns[3:]:\n",
        "        df[col] = df[col].shift(1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def create_df_clouds(n: list[tuple], hour: int, coord: tuple) -> tuple:\n",
        "    global clouds\n",
        "    df = pd.DataFrame(data=[['0/0', h + 1] + [clouds[h][coord[0]][coord[1]]] + [clouds[h][i[0]][i[1]] for i in n] +\n",
        "         [clouds[h][i[0]][i[1]] for i in n] + [clouds[h][i[0]][i[1]] for i in n] + [clouds[h][i[0]][i[1]] for i in n] + [clouds[h][i[0]][i[1]] for i in n] for h in range(43 + hour - 1)],\n",
        "                    columns=['unique_id', 'ds', 'y'] +  [f'neibr{i + 1}_cloud' for i in range(24)] + [f'neibr{i + 1}_cloud_shift2' for i in range(24)] +\n",
        "                          [f'neibr{i + 1}_cloud_shift3' for i in range(24)] + [f'neibr{i + 1}_cloud_shift4' for i in range(24)] + [f'neibr{i + 1}_cloud_shift5' for i in range(24)])\n",
        "    df['y'] = double_exponential_smoothing(df.y, 0.3, 0.2)\n",
        "    future_df = df.iloc[[-1], :].copy()\n",
        "    future_df['ds'] = 43 + hour\n",
        "    for i in df.columns[3:3+24]:\n",
        "        df[i] = df[i].shift(1)\n",
        "    for i in df.columns[3+24:3+24*2]:\n",
        "        df[i] = df[i].shift(2)\n",
        "    for i in df.columns[3+24*2:3+24*3]:\n",
        "        df[i] = df[i].shift(3)\n",
        "    for i in df.columns[3+24*3:3+24*4]:\n",
        "        df[i] = df[i].shift(4)\n",
        "    for i in df.columns[3+24*4:]:\n",
        "        df[i] = df[i].shift(5)\n",
        "    return df, future_df\n",
        "\n",
        "\n",
        "def create_df_pressure(n: list[tuple], hour: int, coord: tuple) -> tuple:\n",
        "    df = pd.DataFrame(data=[['0/0', h + 1, pressure[h][coord[0]][coord[1]], pressure[h][n[0][0]][n[0][1]], pressure[h][n[1][0]][n[1][1]], pressure[h][n[2][0]][n[2][1]],\n",
        "                            pressure[h][n[3][0]][n[3][1]], pressure[h][n[4][0]][n[4][1]], pressure[h][n[5][0]][n[5][1]], pressure[h][n[6][0]][n[6][1]],\n",
        "                            pressure[h][n[7][0]][n[7][1]]]  for h in range(43 + hour - 1)],\n",
        "            columns=['unique_id', 'ds', 'y', 'neibr1', 'neibr2', 'neibr3', 'neibr4', 'neibr5', 'neibr6', 'neibr7', 'neibr8'])\n",
        "    future_df = df.iloc[[-1], :].copy()\n",
        "    future_df['ds'] = 43 + hour\n",
        "    for col in df.columns[3:]:\n",
        "        df[col] = df[col].shift(1)\n",
        "    return df, future_df\n",
        "\n",
        "\n",
        "#@jit(nopython=True, cache=True)\n",
        "def create_df_humidity(n: list[tuple], hour: int, coord: tuple) -> tuple:\n",
        "    df = pd.DataFrame(data=[['0/0', h + 1, humidity[h][coord[0]][coord[1]], humidity[h][n[0][0]][n[0][1]], humidity[h][n[1][0]][n[1][1]], humidity[h][n[2][0]][n[2][1]],\n",
        "                            humidity[h][n[3][0]][n[3][1]], humidity[h][n[4][0]][n[4][1]], humidity[h][n[5][0]][n[5][1]], humidity[h][n[6][0]][n[6][1]],\n",
        "                            humidity[h][n[7][0]][n[7][1]]]  for h in range(43 + hour - 1)],\n",
        "            columns=['unique_id', 'ds', 'y', 'neibr1', 'neibr2', 'neibr3', 'neibr4', 'neibr5', 'neibr6', 'neibr7', 'neibr8'])\n",
        "    future_df = df.iloc[[-1], :].copy()\n",
        "    future_df['ds'] = 43 + hour\n",
        "    for col in df.columns[3:]:\n",
        "        df[col] = df[col].shift(1)\n",
        "    return df, future_df\n",
        "\n",
        "\n",
        "#@jit(nopython=True, cache=True)\n",
        "def create_df_temps(n: list[tuple], hour: int, coord: tuple) -> tuple:\n",
        "    df = pd.DataFrame(data=[['0/0', h + 1, temps[h][coord[0]][coord[1]]] + [temps[h][i[0]][i[1]] for i in n] + [temps[h][i[0]][i[1]] for i in n]  for h in range(43 + hour - 1)],\n",
        "        columns=['unique_id', 'ds', 'y', 'neibr1', 'neibr2', 'neibr3', 'neibr4', 'neibr5', 'neibr6', 'neibr7', 'neibr8',\n",
        "         'neibr1_shift2', 'neibr2_shift2', 'neibr3_shift2', 'neibr4_shift2', 'neibr5_shift2', 'neibr6_shift2', 'neibr7_shift2', 'neibr8_shift2'])\n",
        "    df.y = double_exponential_smoothing(df.y, 0.4, 0.2)\n",
        "    future_df = df.iloc[[-1], :].copy()\n",
        "    future_df['ds'] = 43 + hour\n",
        "    for col in df.columns[3:11]:\n",
        "        df[col] = df[col].shift(1)\n",
        "    for col in df.columns[11:19]:\n",
        "        df[col] = df[col].shift(2)\n",
        "    return df, future_df"
      ],
      "metadata": {
        "id": "xQ80iOl9a1JI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlf = MLForecast(\n",
        "    models=[CatBoostRegressor(iterations=800, max_depth=5, verbose=False)],\n",
        "    freq=1,\n",
        "    lags=[1, 2, 3, 4],\n",
        "    lag_transforms = {\n",
        "        1:  [expanding_mean],\n",
        "        2: [expanding_mean],\n",
        "    },\n",
        ")\n",
        "\n",
        "mlf_forecast_clouds = MLForecast(\n",
        "    models=[CatBoostRegressor(iterations=800, max_depth=5, verbose=False)],\n",
        "    freq=1,\n",
        "    lags=[1, 2, 3, 4, 5],\n",
        "    lag_transforms = {\n",
        "        1:  [expanding_mean],\n",
        "        2: [expanding_mean],\n",
        "        3: [ExponentiallyWeightedMean(alpha=0.1)],\n",
        "    },\n",
        "    target_transforms=[LocalStandardScaler()]#, Differences([1])]\n",
        ")\n",
        "\n",
        "mlf_forecast_clouds2 = MLForecast(\n",
        "    models=[CatBoostRegressor(iterations=700, max_depth=5, verbose=False, random_state=3)],\n",
        "    freq=1,\n",
        "    lags=list(range(1, 5)),\n",
        "    lag_transforms = {\n",
        "        1:  [expanding_mean],\n",
        "        2: [expanding_mean],\n",
        "        3: [ExponentiallyWeightedMean(alpha=0.2)],\n",
        "    },\n",
        "    target_transforms=[LocalStandardScaler()]#, Differences([1])]\n",
        ")\n",
        "\n",
        "mlf_temps = MLForecast(\n",
        "    models=[CatBoostRegressor(iterations=800, max_depth=5, verbose=False, random_state=3)],\n",
        "    freq=1,\n",
        "    lags=[1, 2, 3, 4],\n",
        "    lag_transforms = {\n",
        "        1:  [expanding_mean, ExponentiallyWeightedMean(0.3)],\n",
        "        2: [expanding_mean, RollingMean(window_size=5)],\n",
        "        #3: [RollingMean(window_size=5)]\n",
        "    },\n",
        ")\n",
        "\n",
        "mlf_forecast_clouds3 = MLForecast(\n",
        "    models=[CatBoostRegressor(iterations=1200, max_depth=4, verbose=False, random_state=3, l2_leaf_reg=1)],\n",
        "    freq=1,\n",
        "    lags=list(range(1, 5)),\n",
        "    lag_transforms = {\n",
        "        1:  [expanding_mean],\n",
        "        2: [expanding_mean, RollingMean(window_size=5)],\n",
        "        3: [ExponentiallyWeightedMean(alpha=0.2)],\n",
        "    },\n",
        "    target_transforms=[LocalStandardScaler()]#, Differences([1])]\n",
        ")\n",
        "\n",
        "\n",
        "def double_exponential_smoothing(series, alpha, beta):\n",
        "    result = [series[0]]\n",
        "    for n in range(1, len(series)+1):\n",
        "        if n == 1:\n",
        "            level, trend = series[0], series[1] - series[0]\n",
        "        if n >= len(series):\n",
        "            value = result[-1]\n",
        "        else:\n",
        "            value = series[n]\n",
        "        last_level, level = level, alpha*value + (1-alpha)*(level+trend)\n",
        "        trend = beta*(level-last_level) + (1-beta)*trend\n",
        "        result.append(level+trend)\n",
        "    return result[:-1]"
      ],
      "metadata": {
        "id": "iKJqLE677L95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_all_forecast_clouds():\n",
        "    global clouds\n",
        "    for hour in tqdm(range(1, 6)):\n",
        "        preds = make_current_hour_preds(hour, 'clouds')\n",
        "        clouds = clouds.tolist()\n",
        "        clouds.append([[preds[30 * row + col] for col in range(30)] for row in range(30)])\n",
        "        clouds = np.array(clouds)\n",
        "    return clouds[-5:]\n",
        "\n",
        "\n",
        "def make_all_forecast_humidity():\n",
        "    global humidity\n",
        "    for hour in tqdm(range(1, 6)):\n",
        "        preds = make_current_hour_preds(hour, 'humidity')\n",
        "        humidity = humidity.tolist()\n",
        "        humidity.append([[preds[30 * row + col] for col in range(30)] for row in range(30)])\n",
        "        humidity = np.array(humidity)\n",
        "    return humidity[-5:]\n",
        "\n",
        "\n",
        "#@jit(nopython=True, cache=True)\n",
        "def make_all_forecast_pressure():\n",
        "    global pressure\n",
        "    for hour in tqdm(range(1, 6)):\n",
        "        preds = make_current_hour_preds(hour, 'press')\n",
        "        pressure = pressure.tolist()\n",
        "        pressure.append([[preds[30 * row + col] for col in range(30)] for row in range(30)])\n",
        "        pressure = np.array(pressure)\n",
        "    return pressure[-5:]\n",
        "\n",
        "\n",
        "#@jit(nopython=True, cache=True)\n",
        "def make_all_forecast_temps():\n",
        "    global temps\n",
        "    for hour in tqdm(range(1, 6)):\n",
        "        preds = make_current_hour_preds(hour, 'temp')\n",
        "        temps = temps.tolist()\n",
        "        temps.append([[preds[30 * row + col] for col in range(30)] for row in range(30)])\n",
        "        temps = np.array(temps)\n",
        "    return temps[-5:]"
      ],
      "metadata": {
        "id": "IiEinSgKizoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "\n",
        "#@jit(nopython=True)\n",
        "def make_current_hour_preds(hour: int, pred_type: str) -> list[float]:\n",
        "    preds = []\n",
        "    for row in tqdm(range(30)):\n",
        "        for col in range(30):\n",
        "            n = find_nine_closest_neighbors(coordinates, (row, col))\n",
        "            res = 0\n",
        "            if pred_type == 'temp':\n",
        "                df, future_df = create_df_temps(n, hour, (row, col))\n",
        "                res = prediction_temp(df, future_df, hour, (row, col))\n",
        "            elif pred_type == 'press':\n",
        "                df, future_df = create_df_pressure(n, hour, (row, col))\n",
        "                res = prediction(df, future_df, hour, (row, col))\n",
        "            elif pred_type == 'humidity':\n",
        "                df, future_df = create_df_humidity(n, hour, (row, col))\n",
        "                res = prediction(df, future_df, hour, (row, col))\n",
        "            elif pred_type == 'clouds':\n",
        "                df, future_df = create_df_clouds(n, hour, (row, col))\n",
        "                res = prediction_clouds(df, future_df, hour, (row, col))\n",
        "            preds.append(res)\n",
        "    return preds\n",
        "\n",
        "\n",
        "def prediction_temp(df: pd.DataFrame, future_df: pd.DataFrame, hour: int, coord: tuple) -> float:\n",
        "    data_train = df.iloc[:43 + hour, :]# + hour, :]\n",
        "    mlf_temps.fit(data_train, dropna=True, static_features=[])\n",
        "    ans_df = mlf_temps.predict(h=1, X_df=future_df.drop(columns=['y']))\n",
        "    return ans_df.CatBoostRegressor.values[0]\n",
        "\n",
        "\n",
        "def prediction_clouds(df: pd.DataFrame, future_df: pd.DataFrame, hour: int, coord: tuple) -> float:\n",
        "    data_train = df.iloc[:43 + hour, :]\n",
        "    #if coord[0] == 0 and coord[1] == 0:\n",
        "    if coord[0] % 10 == 0 and coord[1] % 10 == 0:\n",
        "        mlf_forecast_clouds4.fit(data_train, dropna=True, static_features=[])\n",
        "    ans_df = mlf_forecast_clouds4.predict(h=1, X_df=future_df.drop(columns=['y']))\n",
        "    return ans_df.model1.values[0]\n",
        "\n",
        "\n",
        "def prediction(df: pd.DataFrame, future_df: pd.DataFrame, hour: int, coord: tuple) -> float:\n",
        "    #if hour == 1:\n",
        "    data_train = df.iloc[:43 + hour, :]# + hour, :]\n",
        "    mlf.fit(data_train, dropna=True, static_features=[])\n",
        "        #with open(f\"/content/models/temps_model_{coord[0]}_{coord[1]}.pkl\", \"wb\") as f:\n",
        "        #    pickle.dump(mlf, f)\n",
        "    ans_df = mlf.predict(h=1, X_df=future_df.drop(columns=['y']))\n",
        "    return ans_df.CatBoostRegressor.values[0]\n",
        "    #else:\n",
        "    #    with open(f\"/content/models/temps_model_{coord[0]}_{coord[1]}.pkl\", \"rb\") as f:\n",
        "    #        mlf = pickle.load(f)\n",
        "    #    ans_df = mlf.predict(h=1, X_df=future_df.drop(columns=['y']))\n",
        "    #    return ans_df.CatBoostRegressor.values[0]\n",
        "#mean_absolute_percentage_error(data_val['y'], ans_df.CatBoostRegressor)\n",
        "\n",
        "\n",
        "def get_neibrs(row: int, col: int) -> list[tuple]:\n",
        "    if row == 0 and col == 0:\n",
        "        n = [(0, 1), (1, 0), (1, 1), (2, 1), (2, 2), (1, 2), (2, 0), (0, 2)]\n",
        "    elif row == 29 and col == 0:\n",
        "        n = [(29, 1), (28, 0), (28, 1), (27, 0), (27, 1), (27, 2), (28, 2), (29, 2)]\n",
        "    elif row == 0 and col == 29:\n",
        "        n = [(0, 28), (1, 28), (1, 29), (0, 27), (1, 27), (2, 27), (2, 28), (2, 29)]\n",
        "    elif row == 29 and col == 29:\n",
        "        n = [(29, 28), (28, 28), (28, 29), (29, 27), (28, 27), (27, 27), (27, 28), (27, 29)]\n",
        "    elif row == 0 and col != 29 and col != 0:\n",
        "        n = [(row, col - 1), (row, col + 1), (row + 1, col - 1), (row + 1, col), (row + 1, col + 1), (row + 2, col - 1), (row + 2, col), (row + 2, col + 1)]\n",
        "    elif row == 29 and col != 29 and col != 0:\n",
        "        n = [(row, col - 1), (row, col + 1), (row - 1, col - 1), (row - 1, col), (row - 1, col + 1), (row - 2, col - 1), (row - 2, col), (row - 2, col + 1)]\n",
        "    elif col == 0 and row != 29 and row != 0:\n",
        "        n = [(row + 1, col), (row - 1, col), (row + 1, col + 1), (row, col + 1), (row - 1, col + 1), (row + 1, col + 2), (row, col + 2), (row - 1, col + 2)]\n",
        "    elif col == 29 and row != 29 and row != 0:\n",
        "        n = [(row + 1, col), (row - 1, col), (row + 1, col - 1), (row, col - 1), (row - 1, col - 1), (row + 1, col - 2), (row, col - 2), (row - 1, col - 2)]\n",
        "    else:\n",
        "        n = [(row - 1, col - 1), (row - 1, col), (row - 1, col + 1), (row, col - 1), (row, col + 1), (row + 1, col - 1), (row + 1, col), (row + 1, col + 1)]\n",
        "    return n\n"
      ],
      "metadata": {
        "id": "8_yU-d6jvzTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clouds.shape"
      ],
      "metadata": {
        "id": "ajVowqMq3QMZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "make_all_forecast_clouds()"
      ],
      "metadata": {
        "id": "ySGFpX-CiCLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Submit"
      ],
      "metadata": {
        "id": "_nduHgY4FtpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clouds.shape"
      ],
      "metadata": {
        "id": "87LVcTnaUe-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.median(t[43].reshape(-1)), np.median(t[44].reshape(-1)), np.median(t[45].reshape(-1)), np.median(t[46].reshape(-1)), np.median(t[47].reshape(-1))"
      ],
      "metadata": {
        "id": "Vy-sYmZ9c956"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.median(clouds[43].reshape(-1)), np.median(clouds[44].reshape(-1)), np.median(clouds[45].reshape(-1)), np.median(clouds[46].reshape(-1)), np.median(clouds[47].reshape(-1))"
      ],
      "metadata": {
        "id": "LHJR2ek7nJV7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.save('/content/clouds_forecast5_12.npy', clouds)"
      ],
      "metadata": {
        "id": "HWAiuaII8Q9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = np.load('/content/clouds_forecast5_12.npy')\n",
        "t.shape"
      ],
      "metadata": {
        "id": "vXCFoaXZ8WMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solution = pd.read_csv('/content/drive/MyDrive/YADRO_Hack/solution_best2.csv')\n",
        "solution.head()"
      ],
      "metadata": {
        "id": "5I20idPa7XXg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clouds_forecast5 = [clouds[43], clouds[44], clouds[45], clouds[46], clouds[47]]\n",
        "clouds_forecast5 = np.array(clouds_forecast5)\n",
        "clouds_forecast5.shape"
      ],
      "metadata": {
        "id": "h9M3xYOz7m9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clouds_forecast5 = clouds_forecast5.reshape(-1)\n",
        "clouds_forecast5.shape, clouds_forecast5[:-5]"
      ],
      "metadata": {
        "id": "2_wlQTXn7r-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solution['cloud_cover'] = clouds_forecast5\n",
        "solution.head()"
      ],
      "metadata": {
        "id": "gQgXybP37u3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solution.head()"
      ],
      "metadata": {
        "id": "-X47nbjw4pzU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "solution.to_csv('/content/solution_best2_update_clouds9.csv', index=False)"
      ],
      "metadata": {
        "id": "JkoP6DKB73kq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments clouds"
      ],
      "metadata": {
        "id": "T59EzN0Cv2s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#169 стационарных рядов"
      ],
      "metadata": {
        "id": "j22xpJInkT6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "\n",
        "coordinates = [(i, j) for i in range(30) for j in range(30)]\n",
        "\n",
        "\n",
        "def distance(point1, point2):\n",
        "    return math.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2) * 5\n",
        "\n",
        "# Функция для поиска 24 ближайших соседей заданной точки\n",
        "def find_nine_closest_neighbors(coordinates, target):\n",
        "    neighbors = []\n",
        "    min_neighbors = [(0, 0)] + [(0, 0)] * 24\n",
        "    min_distances = [float('inf')] + [float('inf')] * 24\n",
        "\n",
        "    for coord in coordinates:\n",
        "        dist = distance(coord, target)\n",
        "        if dist < max(min_distances):\n",
        "            idx = min_distances.index(max(min_distances))\n",
        "            min_distances[idx] = dist\n",
        "            min_neighbors[idx] = coord\n",
        "\n",
        "    for i in range(24):\n",
        "        neighbors.append(min_neighbors[i])\n",
        "    return sorted(neighbors, key=lambda x: (x[0] - target[0]) ** 2 + (x[1] - target[1]) ** 2)\n"
      ],
      "metadata": {
        "id": "MwabXwKNe5Et"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "dN3xmd9zKB32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#добавим соседей и будем смотреть только tempreture\n",
        "from tqdm import tqdm\n",
        "hour = 43\n",
        "arr3 = []\n",
        "mean_mape = 0\n",
        "for row in tqdm(range(30)):\n",
        "    for col in range(30):\n",
        "        n = find_nine_closest_neighbors(coordinates, (row, col))\n",
        "        df = pd.DataFrame(data=[['0/0', h + 1] + [clouds[h][row][col]] + [clouds[h][i[0]][i[1]] for i in n] +\n",
        "         [clouds[h][i[0]][i[1]] for i in n] + [clouds[h][i[0]][i[1]] for i in n] + [clouds[h][i[0]][i[1]] for i in n] + [clouds[h][i[0]][i[1]] for i in n] for h in range(43)],\n",
        "                    columns=['unique_id', 'ds', 'y'] +  [f'neibr{i + 1}_cloud' for i in range(24)] + [f'neibr{i + 1}_cloud_shift2' for i in range(24)] +\n",
        "                          [f'neibr{i + 1}_cloud_shift3' for i in range(24)] + [f'neibr{i + 1}_cloud_shift4' for i in range(24)] + [f'neibr{i + 1}_cloud_shift5' for i in range(24)])\n",
        "        for i in df.columns[3:3+24]:\n",
        "            df[i] = df[i].shift(1)\n",
        "        for i in df.columns[3+24:3+24*2]:\n",
        "            df[i] = df[i].shift(2)\n",
        "        for i in df.columns[3+24*2:3+24*3]:\n",
        "            df[i] = df[i].shift(3)\n",
        "        for i in df.columns[3+24*3:3+24*4]:\n",
        "            df[i] = df[i].shift(4)\n",
        "        for i in df.columns[3+24*4:]:\n",
        "            df[i] = df[i].shift(5)\n",
        "        df['y'] = double_exponential_smoothing(df.y, 0.3, 0.2)\n",
        "        data_train = df.iloc[:41, :]\n",
        "        data_val = df.iloc[41:, :]\n",
        "        if row % 10 == 0 and col % 10 == 0:\n",
        "            mlf_forecast_clouds4.fit(data_train, dropna=True, static_features=[])\n",
        "        #if row == 0 and col == 0:\n",
        "        #    mlf_forecast_clouds2.fit(data_train, dropna=True, static_features=[])\n",
        "        ans_df = mlf_forecast_clouds4.predict(h=2, X_df=data_val.drop(columns=['y']))\n",
        "        arr3.append(ans_df.model1.values.reshape(-1))#, ans_df.model2.values.reshape(-1)])\n",
        "        mean_mape += mean_absolute_percentage_error(data_val['y'], ans_df.model1)#(ans_df.model2 + ans_df.model1) / 2)\n",
        "mean_mape / 900"
      ],
      "metadata": {
        "id": "23O14qYA4ISR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import stats\n",
        "np.array(arr3).mean(), np.median(np.array(arr3))"
      ],
      "metadata": {
        "id": "BNWRD4H__Cjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0.1914269615058467 (31.401661870978295, 31.062088936896615"
      ],
      "metadata": {
        "id": "Z4WyEXgolxeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlf_forecast_clouds3 = MLForecast(\n",
        "    models={'model1': CatBoostRegressor(iterations=300, max_depth=3, verbose=False, random_state=3, loss_function='Quantile:alpha=0.2')},\n",
        "            #'model2': XGBRegressor(max_depth=5, random_state=3, objective='reg:quantileerror', quantile_alpha=0.85)},\n",
        "    freq=1,\n",
        "    lags=list(range(1, 5)),\n",
        "    lag_transforms = {\n",
        "        1:  [expanding_mean],\n",
        "        2: [expanding_mean, RollingMean(window_size=5)],\n",
        "        3: [RollingMean(window_size=5)],\n",
        "    },\n",
        "    target_transforms=[LocalStandardScaler()]#, Differences([1])]\n",
        ")"
      ],
      "metadata": {
        "id": "Apx2WT9hWM_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlf_forecast_clouds4 = MLForecast(\n",
        "    models={'model1': CatBoostRegressor(iterations=400, max_depth=4, verbose=False, random_state=3, l2_leaf_reg=0.5, loss_function='Quantile:alpha=0.1')},\n",
        "            #'model2': XGBRegressor(max_depth=5, random_state=3, objective='reg:quantileerror', quantile_alpha=0.85)},\n",
        "    freq=1,\n",
        "    lags=list(range(1, 5)),\n",
        "    lag_transforms = {\n",
        "        1:  [expanding_mean],\n",
        "        2: [expanding_mean, RollingMean(window_size=3)],\n",
        "        3: [expanding_mean, RollingMean(window_size=3)],\n",
        "    },\n",
        "    target_transforms=[LocalStandardScaler()]#, Differences([1])]\n",
        ")"
      ],
      "metadata": {
        "id": "SGvJ1okoHucI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a, b = 0, 0\n",
        "n = find_nine_closest_neighbors(coordinates, (a, b))\n",
        "df = pd.DataFrame(data=[['0/0', h + 1] + [clouds[h][a][b]] + [clouds[h][i[0]][i[1]] for i in n] +\n",
        "         [clouds[h][i[0]][i[1]] for i in n] for h in range(43)],\n",
        "                    columns=['unique_id', 'ds', 'y'] +  [f'neibr{i + 1}_cloud' for i in range(35)] + [f'neibr{i + 1}_cloud_shift2' for i in range(35)])\n",
        "for col in df.columns[3:3+35]:\n",
        "    df[col] = df[col].shift(1)\n",
        "for col in df.columns[3+35:]:\n",
        "    df[col] = df[col].shift(2)\n",
        "df['y'] = double_exponential_smoothing(df.y, 0.3, 0.2)"
      ],
      "metadata": {
        "id": "y7cNVV5B8-6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "tq_dxFaV8VuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = df.iloc[:41, :]\n",
        "data_val = df.iloc[41:, :]"
      ],
      "metadata": {
        "id": "0HcWWoZqrNQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlf_forecast_clouds4.fit(data_train, dropna=True, static_features=[])"
      ],
      "metadata": {
        "id": "iRKM3yx27h98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_df = mlf_forecast_clouds2.predict(h=5, X_df=data_val.drop(columns=['y']))\n",
        "ans_df.CatBoostRegressor"
      ],
      "metadata": {
        "id": "18ABNTCkrHAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_absolute_percentage_error(data_val['y'], ans_df.CatBoostRegressor)"
      ],
      "metadata": {
        "id": "UhR79XktqKfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.y.plot()"
      ],
      "metadata": {
        "id": "aFncf60wT9Ak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['y_smooth'] = double_exponential_smoothing(df.y, 0.4, 0.2)\n",
        "df.y_smooth.plot()"
      ],
      "metadata": {
        "id": "6C4n9vT3T_Qx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments temps"
      ],
      "metadata": {
        "id": "o0EKKOBDR6_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#добавим соседей и будем смотреть только tempreture\n",
        "hour = 43\n",
        "mean_mape = 0\n",
        "for row in tqdm(range(0, 30, 6)):\n",
        "    for col in range(30):\n",
        "        n = get_neibrs(row, col)\n",
        "        df = pd.DataFrame(data=[['0/0', h + 1, temps[h][row][col]] + [temps[h][i[0]][i[1]] for i in n] + [temps[h][i[0]][i[1]] for i in n]  for h in range(43)],\n",
        "        columns=['unique_id', 'ds', 'y', 'neibr1', 'neibr2', 'neibr3', 'neibr4', 'neibr5', 'neibr6', 'neibr7', 'neibr8',\n",
        "         'neibr1_shift2', 'neibr2_shift2', 'neibr3_shift2', 'neibr4_shift2', 'neibr5_shift2', 'neibr6_shift2', 'neibr7_shift2', 'neibr8_shift2'])\n",
        "        for col in df.columns[3:11]:\n",
        "            df[col] = df[col].shift(1)\n",
        "        for col in df.columns[11:19]:\n",
        "            df[col] = df[col].shift(2)\n",
        "        df.y = double_exponential_smoothing(df.y, 0.4, 0.2)\n",
        "        data_train = df.iloc[:38, :]\n",
        "        data_val = df.iloc[38:, :]\n",
        "        mlf_temps.fit(data_train, dropna=True, static_features=[])\n",
        "        ans_df = mlf_temps.predict(h=5, X_df=data_val.drop(columns=['y']))\n",
        "        mean_mape += mean_absolute_percentage_error(data_val['y'], ans_df.CatBoostRegressor)\n",
        "mean_mape / 150"
      ],
      "metadata": {
        "id": "T-Q90YtpSHxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#0.04695709071939297"
      ],
      "metadata": {
        "id": "2edPRg8RdcJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlf_temps = MLForecast(\n",
        "    models=[CatBoostRegressor(iterations=800, max_depth=5, verbose=False, random_state=3)],\n",
        "    freq=1,\n",
        "    lags=[1, 2, 3, 4],\n",
        "    lag_transforms = {\n",
        "        1:  [expanding_mean, ExponentiallyWeightedMean(0.3)],\n",
        "        2: [expanding_mean, RollingMean(window_size=5)],\n",
        "        #3: [RollingMean(window_size=5)]\n",
        "    },\n",
        ")"
      ],
      "metadata": {
        "id": "FE5F9kQXSKFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = get_neibrs(0, 0)\n",
        "df = pd.DataFrame(data=[['0/0', h + 1, temps[h][0][0]] + [temps[h][i[0]][i[1]] for i in n] +\n",
        "    [temps[h][i[0]][i[1]] for i in n] + [temps[h][i[0]][i[1]] for i in n] for h in range(43)],\n",
        "columns=['unique_id', 'ds', 'y', 'neibr1', 'neibr2', 'neibr3', 'neibr4', 'neibr5', 'neibr6', 'neibr7', 'neibr8',\n",
        "         'neibr1_shift2', 'neibr2_shift2', 'neibr3_shift2', 'neibr4_shift2', 'neibr5_shift2', 'neibr6_shift2', 'neibr7_shift2', 'neibr8_shift2',\n",
        "         'neibr1_shift3', 'neibr2_shift3', 'neibr3_shift3', 'neibr4_shift3', 'neibr5_shift3', 'neibr6_shift3', 'neibr7_shift3', 'neibr8_shift3'])\n",
        "#df.y = double_exponential_smoothing(df.y, 0.3, 0.2)\n",
        "for col in df.columns[3:11]:\n",
        "    df[col] = df[col].shift(1)\n",
        "for col in df.columns[11:19]:\n",
        "    df[col] = df[col].shift(2)\n",
        "for col in df.columns[19:27]:\n",
        "    df[col] = df[col].shift(3)"
      ],
      "metadata": {
        "id": "DQlYkK-PR9g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.y.plot()"
      ],
      "metadata": {
        "id": "keRP7XG2fcEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['y2'] = double_exponential_smoothing(df.y, 0.5, 0.5)\n",
        "df.y2.plot()"
      ],
      "metadata": {
        "id": "IoomvQi1fdiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_train = df.iloc[:38, :]\n",
        "data_val = df.iloc[38:, :]"
      ],
      "metadata": {
        "id": "k4RbkRknSMnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlf_temps.fit(data_train, dropna=True, static_features=[])"
      ],
      "metadata": {
        "id": "BmcwyxllSOId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ans_df = mlf_temps.predict(h=5, X_df=data_val.drop(columns=['y']))\n",
        "ans_df.CatBoostRegressor, mean_absolute_percentage_error(data_val['y'], ans_df.CatBoostRegressor)"
      ],
      "metadata": {
        "id": "ZhH915C_SPjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SARIMAX"
      ],
      "metadata": {
        "id": "JdHrtb61uR9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pmdarima -q"
      ],
      "metadata": {
        "id": "K5lOrQ_p_a_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pmdarima as pm"
      ],
      "metadata": {
        "id": "-dAIgQOw_Zzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "temps1['neibr1'].shift(1)"
      ],
      "metadata": {
        "id": "U8wVC-W8BpZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 38\n",
        "data_train = temps1.iloc[:n, :]\n",
        "data_val = temps1.iloc[n:, :]"
      ],
      "metadata": {
        "id": "mDbB22M-_qXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "exog = ['neibr1', 'neibr2', 'neibr3', 'neibr4', 'neibr5', 'neibr6', 'neibr7', 'neibr8']"
      ],
      "metadata": {
        "id": "JWSbM7fDAHWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SARIMAX_model = pm.auto_arima(data_train[['y']], exogenous=data_train[exog],\n",
        "                           start_p=1, start_q=1,\n",
        "                           test='adf',\n",
        "                           max_p=3, max_q=3, m=12,\n",
        "                           start_P=0, seasonal=True,\n",
        "                           d=None, D=1,\n",
        "                           trace=False,\n",
        "                           error_action='ignore',\n",
        "                           suppress_warnings=True,\n",
        "                           stepwise=True)"
      ],
      "metadata": {
        "id": "Y0WjlGHq92Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fitted, confint = SARIMAX_model.predict(n_periods=5,\n",
        "                                            return_conf_int=True,\n",
        "                                            exogenous=data_val[exog])\n"
      ],
      "metadata": {
        "id": "ZXzG628qAFVV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "\n",
        "\n",
        "mean_absolute_percentage_error(data_val['y'], fitted)"
      ],
      "metadata": {
        "id": "Zeyu-3a9Ag4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best cloud 0.66\n",
        "\n",
        "\n",
        "```mlf_forecast_clouds3 = MLForecast(\n",
        "    models={'model1': CatBoostRegressor(iterations=1000, max_depth=5, verbose=False, random_state=3, l2_leaf_reg=2, loss_function='Quantile:alpha=0.1')},\n",
        "            #'model2': XGBRegressor(max_depth=5, random_state=3, objective='reg:quantileerror', quantile_alpha=0.85)},\n",
        "    freq=1,\n",
        "    lags=list(range(1, 5)),\n",
        "    lag_transforms = {\n",
        "        1:  [expanding_mean],\n",
        "        2: [expanding_mean, RollingMean(window_size=3)],\n",
        "        3: [expanding_mean, RollingMean(window_size=3)],\n",
        "    },\n",
        "    target_transforms=[LocalStandardScaler()]#, Differences([1])]\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "```n = find_nine_closest_neighbors(coordinates, (a, b))\n",
        "df = pd.DataFrame(data=[['0/0', h + 1] + [clouds[h][a][b]] #+ [clouds[h][i[0]][i[1]] for i in n] + [clouds[h][i[0]][i[1]] for i in n]\n",
        "        + [clouds[h][i[0]][i[1]] for i in n] + [clouds[h][i[0]][i[1]] for i in n]\n",
        "                        for h in range(43)],\n",
        "                    columns=['unique_id', 'ds', 'y', 'neibr1_cloud', 'neibr2_cloud', 'neibr3_cloud', 'neibr4_cloud', 'neibr5_cloud', 'neibr6_cloud',\n",
        "                            'neibr7_cloud', 'neibr8_cloud', 'neibr9_cloud', 'neibr10_cloud', 'neibr11_cloud', 'neibr12_cloud', 'neibr13_cloud', 'neibr14_cloud',\n",
        "                            'neibr15_cloud', 'neibr16_cloud', 'neibr17_cloud', 'neibr18_cloud', 'neibr19_cloud', 'neibr20_cloud', 'neibr21_cloud', 'neibr22_cloud',\n",
        "                            'neibr23_cloud', 'neibr24_cloud',\n",
        "                             'neibr1_cloud_shift2', 'neibr2_cloud_shift2', 'neibr3_cloud_shift2', 'neibr4_cloud_shift2',\n",
        "                            'neibr5_cloud_shift2', 'neibr6_cloud_shift2', 'neibr7_cloud_shift2', 'neibr8_cloud_shift2',\n",
        "                             'neibr9_cloud_shift2', 'neibr10_cloud_shift2', 'neibr11_cloud_shift2', 'neibr12_cloud_shift2',\n",
        "                            'neibr13_cloud_shift2', 'neibr14_cloud_shift2', 'neibr15_cloud_shift2', 'neibr16_cloud_shift2',\n",
        "                             'neibr17_cloud_shift2', 'neibr18_cloud_shift2', 'neibr19_cloud_shift2', 'neibr20_cloud_shift2',\n",
        "                            'neibr21_cloud_shift2', 'neibr22_cloud_shift2', 'neibr23_cloud_shift2', 'neibr24_cloud_shift2',\n",
        "for col in df.columns[3:27]:\n",
        "    df[col] = df[col].shift(1)\n",
        "for col in df.columns[27:27+24]:\n",
        "    df[col] = df[col].shift(2)\n",
        "```"
      ],
      "metadata": {
        "id": "TSSiBT8GD48r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Best Temp 0.054\n",
        "\n",
        "\n",
        "```mlf_temps = MLForecast(\n",
        "    models=[CatBoostRegressor(iterations=800, max_depth=5, verbose=False, random_state=3)],\n",
        "    freq=1,\n",
        "    lags=[1, 2, 3, 4],\n",
        "    lag_transforms = {\n",
        "        1:  [expanding_mean, ExponentiallyWeightedMean(0.3)],\n",
        "        2: [expanding_mean, RollingMean(window_size=5)],\n",
        "        #3: [RollingMean(window_size=5)]\n",
        "    },\n",
        ")```\n",
        "\n",
        "\n",
        "```def create_df_temps(n: list[tuple], hour: int, coord: tuple) -> tuple:\n",
        "    df = pd.DataFrame(data=[['0/0', h + 1, temps[h][coord[0]][coord[1]]] + [temps[h][i[0]][i[1]] for i in n] + [temps[h][i[0]][i[1]] for i in n]  for h in range(43 + hour - 1)],\n",
        "        columns=['unique_id', 'ds', 'y', 'neibr1', 'neibr2', 'neibr3', 'neibr4', 'neibr5', 'neibr6', 'neibr7', 'neibr8',\n",
        "         'neibr1_shift2', 'neibr2_shift2', 'neibr3_shift2', 'neibr4_shift2', 'neibr5_shift2', 'neibr6_shift2', 'neibr7_shift2', 'neibr8_shift2'])\n",
        "    df.y = double_exponential_smoothing(df.y, 0.4, 0.2)\n",
        "    future_df = df.iloc[[-1], :].copy()\n",
        "    future_df['ds'] = 43 + hour\n",
        "    for col in df.columns[3:11]:\n",
        "        df[col] = df[col].shift(1)\n",
        "    for col in df.columns[11:19]:\n",
        "        df[col] = df[col].shift(2)\n",
        "    return df, future_df\n",
        "```"
      ],
      "metadata": {
        "id": "-QCj6ZjmwzHH"
      }
    }
  ]
}